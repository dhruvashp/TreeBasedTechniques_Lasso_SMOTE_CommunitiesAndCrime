{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW4\n",
    "2f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the process :\n",
    "\n",
    "SMOTE is considered part of training\n",
    "\n",
    "Thus our method will be thus:\n",
    "\n",
    "We'll use 5 fold CV on the training set (original training set)\n",
    "\n",
    "Next, we'll have generated in each iteration of our 5-fold CV:\n",
    "A CV train and CV test set\n",
    "Since test set (validation) of CV is used to estimate parameters of the final test set, we will keep this test/validation set untouched and won't SMOTE on it as the final test set obviously won't either be SMOTE'd (assumed)\n",
    "\n",
    "The reason for this is simple : We assume that since train, validation and test sets are large enough, when we split train into train and validation, the validation set is a decent representation of the test set in terms of containing approximately the same ratio of classes as the test set\n",
    "\n",
    "Thus SMOTE'ing the validation set and then obtaining prediction, test error estimates, obviously doesn't make sense\n",
    "\n",
    "THUS, Validation set WILL NOT be SMOTE'd\n",
    "      Train set in a CV iteration, in each CV iteration, will be SMOTE'd\n",
    "      Test set obviously assumed to not have been SMOTE'd\n",
    "\n",
    "The model will be fit on this SMOTE'd train set in each CV iteration (total 5) post which the test errors estimated via the un-SMOTE'd validation set, and then the model finally predicting the, again, un-SMOTE'd final test set.\n",
    "\n",
    "This procedure will be followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imblearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import weka.core.jvm as jvm\n",
    "from weka.classifiers import Classifier\n",
    "from weka.core.converters import Loader, Saver\n",
    "from weka.classifiers import Classifier, Evaluation\n",
    "from weka.core.classes import Random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from weka.filters import Filter\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from weka.core.dataset import create_instances_from_lists\n",
    "from weka.core.dataset import create_instances_from_matrices\n",
    "jvm.start()\n",
    "jvm.start(packages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('aps_train.csv',index_col=0)\n",
    "df_test = pd.read_csv('aps_test.csv',index_col=0)\n",
    "X_train = df_train.drop(columns=['class'])\n",
    "X_test = df_test.drop(columns=['class'])\n",
    "y_train = df_train['class']\n",
    "y_test = df_test['class']\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing Cross Validation for test error estimation\n",
    "\n",
    "kf = KFold(n_splits=5) # We will do 5 fold CV\n",
    "test_error_cv_array = np.zeros(5)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for train_cv_index, test_cv_index in kf.split(X_train):\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_cv_index,:], X_train.iloc[test_cv_index,:]\n",
    "    y_train_cv, y_test_cv = y_train[train_cv_index], y_train[test_cv_index]\n",
    "    count = count + 1\n",
    "    # SMOTE will be performed on X_train_cv and y_train_cv\n",
    "    \n",
    "    oversample = SMOTE()\n",
    "    X_train_cv_sm, y_train_cv_sm = oversample.fit_resample(X_train_cv, y_train_cv)\n",
    "    \n",
    "    # X_train_cv_sm and y_train_cv_sm contain required SMOTE'd data. We won't perform random undersampling in a pipeline\n",
    "    # format due to runtime constraints\n",
    "    \n",
    "    # X_train_cv_sm & y_train_cv_sm\n",
    "    \n",
    "    dataset = pd.concat([X_train_cv_sm,y_train_cv_sm],axis=1)\n",
    "    \n",
    "    # dataset contains the new data obtained after SMOTE\n",
    "    # note that SMOTE retains column headers of our dataframe\n",
    "    \n",
    "    dataset_array = dataset.to_numpy()\n",
    "    \n",
    "    # this dataset_array now has dataframe as an array, so that weka can read it as a list\n",
    "    \n",
    "    data_interim = create_instances_from_lists(dataset_array)\n",
    "    \n",
    "    discretize_class = Filter(classname = 'weka.filters.unsupervised.attribute.NumericToNominal', options = ['-R', 'last'])\n",
    "    \n",
    "    discretize_class.inputformat(data_interim)\n",
    "    \n",
    "    data = discretize_class.filter(data_interim)\n",
    "    \n",
    "    # this is data read by weka, contains classes at the end\n",
    "    \n",
    "    \n",
    "    \n",
    "    # cls on smote as this is our fit, fit on SMOTE\n",
    "    \n",
    "    cls_on_smote = Classifier(classname=\"weka.classifiers.trees.LMT\")\n",
    "    \n",
    "    data.class_is_last()\n",
    "    \n",
    "    cls_on_smote.build_classifier(data)\n",
    "    \n",
    "    # cls_on_smote has the classification model now, trained on SMOTE'd CV Train\n",
    "    \n",
    "    validate_dataset_array = pd.concat([X_test_cv, y_test_cv], axis=1).to_numpy()  \n",
    "    \n",
    "    # validate_dataset_array is numpy of X_test_cv\n",
    "    \n",
    "    validate_dataset_interim = create_instances_from_lists(validate_dataset_array)\n",
    "    \n",
    "    discretize_class.inputformat(validate_dataset_interim)\n",
    "    \n",
    "    validate_dataset = discretize_class.filter(validate_dataset_interim)\n",
    "    \n",
    "    validate_dataset.class_is_last()\n",
    "    \n",
    "    # validate_dataset contains the instances with only feature data, to predict on\n",
    "    \n",
    "    y_test_cv_predicted = np.zeros(y_test_cv.shape[0])\n",
    "    \n",
    "    for index,inst in enumerate(validate_dataset):\n",
    "        y_test_cv_predicted[index] = cls_on_smote.classify_instance(inst) \n",
    "        \n",
    "    \n",
    "    # y_test_cv_predicted contains predicted y values in a numpy array\n",
    "    \n",
    "    mis = 0\n",
    "    for i in np.arange(0,y_test_cv.shape[0]):\n",
    "        if y_test_cv_predicted[i] != y_test_cv[i]:\n",
    "            mis = mis + 1\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    error_test = (mis/(y_test_cv.shape[0]))*100\n",
    "    \n",
    "    test_error_cv_array[count-1] = error_test   # in percentage\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print('The test error vector for CV (validation error) is, for all CV iterations : \\n',test_error_cv_array)\n",
    "\n",
    "estimated_test_error = np.mean(test_error_cv_array)\n",
    "\n",
    "print('The estimated test error is : \\n', estimated_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
